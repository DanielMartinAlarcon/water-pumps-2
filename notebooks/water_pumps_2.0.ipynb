{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "pd.set_option('display.max_columns', None)  # Unlimited columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('original_data/sample_submission.csv')\n",
    "X_test = pd.read_csv('original_data/test_features.csv')\n",
    "X_train = pd.read_csv('original_data/train_features.csv')\n",
    "y_train = pd.read_csv('original_data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleanup\n",
    "This time around, I'll make a single cleanup function that does all the work.  I also plan to use XGBoost and other classifiers that can handle null values, so I'll only get rid of them when I can replace them with more meaningful stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def wrangle(X):\n",
    "    \"\"\"\n",
    "    All-inclusive data cleanup.\n",
    "    \"\"\"\n",
    " \n",
    "    # Make a clean copy, to ensure we're not changing the original DF\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Looking at all the features with missing values, it looks like those\n",
    "    # features are all categorical variables where 'unknown' would be a\n",
    "    # category we can work with.  I'll replace the NANs accordingly.\n",
    "    X = X.fillna('unknown')\n",
    "    \n",
    "    # Regression on dates won't work.  Instead, I'll turn the \n",
    "    # date_recorded column into the number of years since 2000\n",
    "    # (the earliest date in the training date is from ~2002, and the\n",
    "    # latest from 2013.)\n",
    "    dates = pd.to_datetime(X.date_recorded)\n",
    "    year2000 = pd.to_datetime('2000-01-01')\n",
    "    years = [i.days/365 for i in (dates - year2000)]\n",
    "    X.date_recorded = years\n",
    "    \n",
    "    # region_code and district_code are int64, but they should really be\n",
    "    # treated as categories (and there's only 20-30 classes in each).\n",
    "    # I'll cast them as strings instead.\n",
    "    X.region_code = X.region_code.astype('str')\n",
    "    X.district_code = X.district_code.astype('str')\n",
    "      \n",
    "    # To prevent data conversion warnings, I'll turn all the numerical\n",
    "    # features (except id) into float64.\n",
    "    \n",
    "    # Also, some columns contained bool values and NANs.  \n",
    "    # (e.g., public_meeting, permit)\n",
    "    # I replaced the NANs with strings, which created a problem for later\n",
    "    # operations that don't like heterogeneous datatypes within a single\n",
    "    # column. I'll prevent this problem by casting those two features as str.\n",
    "    \n",
    "    type_dict = {'amount_tsh':'float64',\n",
    "                 'date_recorded':'float64',\n",
    "                 'gps_height':'float64',\n",
    "                 'longitude':'float64',\n",
    "                 'latitude':'float64',\n",
    "                 'num_private':'float64',\n",
    "                 'population':'float64',\n",
    "                 'construction_year':'float64',\n",
    "                 'public_meeting':'str',\n",
    "                 'permit':'str'}\n",
    "    \n",
    "    X = X.astype(dtype = type_dict)\n",
    "    \n",
    "    # Fixing the numerical columns.\n",
    "    # ---------------------------------------------------------------    \n",
    "    \n",
    "    # Numerical columns have several kinds of garbage values that \n",
    "    # Must be replaced with nulls.\n",
    "    numericals = ['amount_tsh',\n",
    "                    'date_recorded',\n",
    "                    'gps_height',\n",
    "                    'longitude',\n",
    "                    'latitude',\n",
    "                    'num_private',\n",
    "                    'population',\n",
    "                    'construction_year']\n",
    "\n",
    "    null_values = {'amount_tsh':0,\n",
    "                     'date_recorded':0,\n",
    "                     'gps_height':0,\n",
    "                     'longitude':0,\n",
    "                     'latitude':-2.000000e-08,\n",
    "                     'num_private':0,\n",
    "                     'population':0,\n",
    "                     'construction_year':0}\n",
    "\n",
    "    # I replace all garbage values with NANs.\n",
    "    for feature, null in null_values.items():\n",
    "        X[feature] = X[feature].replace(null, np.nan)\n",
    "    \n",
    "    # construction_year occasionally claims years far in the future, and \n",
    "    # could presumably also contain years way in the past.  I'll turn anything\n",
    "    # not between 1960 and 2019 into a NAN.\n",
    "    X['construction_year'] = [i if 1960 < i < 2019 else np.nan for i in X['construction_year']]\n",
    "\n",
    "    \n",
    "    # Whenever available, a good replacement value for a NAN is the \n",
    "    # mean or median value for the geographic region around it.\n",
    "\n",
    "    # Replaces the NANs in a ward with the mean of the other rows in that \n",
    "    # same ward. If all the rows in a ward are NANs, though, they remain.\n",
    "    for feature in numericals:\n",
    "        replacements = X.groupby('ward')[feature].transform('mean')\n",
    "        X[feature] = X[feature].fillna(replacements)\n",
    "\n",
    "    # Replaces the NANs in a region with the mean of the other rows in that \n",
    "    # same region (which are much larger than wards)\n",
    "    for feature in numericals:\n",
    "        replacements = X.groupby('region')[feature].transform('mean')\n",
    "        X[feature] = X[feature].fillna(replacements)\n",
    "    \n",
    "    # A few rows are not helped by averaging over the ward or the region.\n",
    "    # Those can stay as they are, with NANs.\n",
    "    \n",
    "    # Fixing the categorical columns.\n",
    "    # ---------------------------------------------------------------    \n",
    "    \n",
    "    # Create list of categorical features\n",
    "    categoricals = X.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "    # Make all strings lowercase, to collapse together some of the categories\n",
    "    X[categoricals] = X[categoricals].applymap(lambda x: x.lower())\n",
    "\n",
    "    # Replace common NAN values\n",
    "    nan_list = ['not known','unknown','none','-','##','not kno','unknown installer']\n",
    "    X = X.replace(nan_list, np.nan)\n",
    "\n",
    "    # Any feature values with fewer than 100 rows gets turned into a NAN\n",
    "    for feature in X[categoricals]:\n",
    "        # Determine which feature values to keep\n",
    "        to_keep = X[feature].value_counts()[X[feature].value_counts() > 100].index.tolist()\n",
    "        # Turn those into NANs (using a copy, to prevent warnings)\n",
    "        feature_copy = X[feature].copy()\n",
    "        feature_copy[~feature_copy.isin(to_keep)] = np.nan\n",
    "        X[feature] = feature_copy\n",
    "    \n",
    "    \n",
    "    garbage = ['extraction_type_group','extraction_type_class',\n",
    "               'region_code','waterpoint_type_group','source_type',\n",
    "              'payment_type','quality_group','quantity_group',\n",
    "              'recorded_by']\n",
    "    \n",
    "    X = X.drop(columns=garbage)\n",
    "    \n",
    "    X['age'] = X['date_recorded'] - X['construction_year']\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def target_encode_cats(X, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Target encodes the categorical features of a dataframe X,\n",
    "    after training the encoder on all the data in X_train and\n",
    "    creating a different encoded column for each feature label\n",
    "    in y_train.\n",
    "    \n",
    "    Target encoding is designed to work with\n",
    "    binary labels; in order to make it work with a vector that has three\n",
    "    values, I target encode against a binary version of each and then\n",
    "    concatenate the results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        Dataset to be fixed\n",
    "        \n",
    "    X_train : Training data to train the encoder.\n",
    "\n",
    "    y_train : pandas.DataFrame\n",
    "                    The vector of training labels\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "            Fixed vector\n",
    "\n",
    "    \"\"\"\n",
    "    # Make a clean copy, to ensure we're not changing the original DF\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Create a list of categorical columns\n",
    "    cats = X_train.select_dtypes(exclude='number').columns.tolist()\n",
    "    \n",
    "    # Make binary lists of all the available labels\n",
    "    y_true = y_train['status_group']\n",
    "    y_works = [1.0 if x == 'functional' else 0.0 for x in y_true]\n",
    "    y_broken = [1.0 if x == 'non functional' else 0.0 for x in y_true]\n",
    "    y_repair = [1.0 if x == 'functional needs repair' else 0.0 for x in y_true]\n",
    "\n",
    "    y_vectors = [y_works, y_broken, y_repair]\n",
    "    X_TE_all = []\n",
    "\n",
    "    # We want to create encoding based on X_train and y_train,\n",
    "    # then apply this encoding to any vector X\n",
    "    for i in [1,2,3]:\n",
    "        # Make an encoder\n",
    "        TE = TargetEncoder()\n",
    "        \n",
    "        # Fit it to the training data\n",
    "        TE.fit(X=X_train[cats], y=y_vectors[i-1])\n",
    "\n",
    "        # Transform the cat columns in X\n",
    "        X_TE = TE.transform(X[cats])\n",
    "        \n",
    "        # Give them custom names, so that the columns encoded against\n",
    "        # each target vector have a different name\n",
    "        X_TE = X_TE.rename(columns=(lambda x: x + '_TE' + str(i)))\n",
    "        X_TE_all.append(X_TE)\n",
    "\n",
    "    new_cats = pd.concat(X_TE_all, sort=False, axis=1)\n",
    "    \n",
    "    X = X.drop(columns=cats)\n",
    "    X = pd.concat([X,new_cats], sort=False, axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 65.5 ms, total: 13.2 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = wrangle(X_train)\n",
    "X_train = target_encode_cats(X_train, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing cleanup steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "modelxgb = XGBClassifier(objective = 'multi:softmax', booster = 'gbtree', \n",
    "                         nrounds = 'min.error.idx', num_class = 3, \n",
    "                         maximize = False, eval_metric = 'merror', eta = .1,\n",
    "                         max_depth = 14, colsample_bytree = .4, n_jobs=-1)\n",
    "\n",
    "y_true = y_train['status_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 249 ms, total: 2min 7s\n",
      "Wall time: 1min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.4, eta=0.1, eval_metric='merror', gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=14, maximize=False,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
       "       nrounds='min.error.idx', nthread=None, num_class=3,\n",
       "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "modelxgb.fit(X_train, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.7 s, sys: 4.75 s, total: 23.5 s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_score = cross_val_score(modelxgb, X_train, y_true, scoring='accuracy', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81920714, 0.81424123, 0.81262626, 0.81203704, 0.81124768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "modelxgb = XGBClassifier(objective = 'multi:softmax', booster = 'dart', \n",
    "                         nrounds = 'min.error.idx', num_class = 3, \n",
    "                         maximize = False, eval_metric = 'merror', eta = .1,\n",
    "                         max_depth = 14, colsample_bytree = .4, n_jobs=-1)\n",
    "\n",
    "y_true = y_train['status_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 16s, sys: 153 ms, total: 7min 16s\n",
      "Wall time: 5min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "       colsample_bytree=0.4, eta=0.1, eval_metric='merror', gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=14, maximize=False,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
       "       nrounds='min.error.idx', nthread=None, num_class=3,\n",
       "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "modelxgb.fit(X_train, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields funder, installer, wpt_name, basin, subvillage, region, district_code, lga, ward, public_meeting, scheme_management, scheme_name, permit, extraction_type, management, management_group, payment, water_quality, quantity, source, source_class, waterpoint_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-50a62e4417b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Predict test data from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Make a dataframe with the answers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \"\"\"\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mtest_dmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mntree_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mntree_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_ntree_limit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    382\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    383\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         data, feature_names, feature_types = _maybe_dt_data(data,\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    239\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    240\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields funder, installer, wpt_name, basin, subvillage, region, district_code, lga, ward, public_meeting, scheme_management, scheme_name, permit, extraction_type, management, management_group, payment, water_quality, quantity, source, source_class, waterpoint_type"
     ]
    }
   ],
   "source": [
    "# Clean test data\n",
    "X_test = wrangle(X_test)\n",
    "X_test = target_encode_cats(X_test, X_train, y_train)\n",
    "\n",
    "# Predict test data from the model\n",
    "y_test_pred = modelxgb.predict(X_test)\n",
    "\n",
    "# Make a dataframe with the answers\n",
    "y_submit = pd.DataFrame({'id':test_features['id'],\n",
    "                         'status_group':y_test_pred} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a submission CSV file\n",
    "y_submit.to_csv('DMA6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
